<!DOCTYPE html>
<html>

<head>
  <!-- Standard Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <!-- Site Properties -->
  <title>TikTalk</title>

  <!-- You MUST include jQuery before Fomantic -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.8/dist/semantic.min.css">
  <script src="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.8/dist/semantic.min.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style type="text/css">
    .hidden.menu {
      display: none;
    }

    .masthead.segment {
      min-height: 700px;
      padding: 0em 0em;  /* munu padding */
    }

    .masthead .logo.item img {
      margin-right: 1em;
    }

    .masthead .ui.menu .ui.button {
      margin-left: 0.5em;
    }

    .masthead h1.ui.header {
      margin-top: 3em;
      margin-bottom: 0em;
      font-size: 4em;
      font-weight: normal;
    }

    .masthead h2 {
      font-size: 1.7em;
      font-weight: normal;
    }

     .ui.vertical.stripe {
      padding: 8em 0em;
    }

    .ui.vertical.stripe h3 {
      font-size: 2em;
    }

    .ui.vertical.stripe .button+h3,
    .ui.vertical.stripe p+h3 {
      margin-top: 3em;
    }

    .ui.vertical.stripe .floated.image {
      clear: both;
    }

    .ui.vertical.stripe p {
      font-size: 1.33em;
    }

    .ui.vertical.stripe .horizontal.divider {
      margin: 3em 0em;
    }

    .quote.stripe.segment {
      padding: 0em;
    }

    .quote.stripe.segment .grid .column {
      padding-top: 5em;
      padding-bottom: 5em;
    }

    .footer.segment {
      padding: 5em 0em;
    }

    .secondary.pointing.menu .toc.item {
      display: none;
    }

    @media only screen and (max-width: 700px) {
      .ui.fixed.menu {
        display: none !important;
      }

      .secondary.pointing.menu .item,
      .secondary.pointing.menu .menu {
        display: none;
      }

      .secondary.pointing.menu .toc.item {
        display: block;
      }

      .masthead.segment {
        min-height: 350px;
      }

      .masthead h1.ui.header {
        font-size: 2em;
        margin-top: 1.5em;
      }

      .masthead h2 {
        margin-top: 0.5em;
        font-size: 1.5em;
      }
    }

    p {
      text-align: justify;
      font-size: 12pt;
    }

    .masthead {
      background-image: url('assets/img/bg.png') !important;
      background-size: cover !important;
    }

    .masthead.segment {
      min-height: 300px;
    }

    .masthead h1.ui.header {
      margin-top: 0em;
    }

    .masthead .ui.tex a {
      margin-bottom: 40px;
    }

    .masthead a {
      color: #EEE;
    }

    .ui.vertical.stripe.segment {
      padding: 5em 0em;
    }
  </style>

  <script>
    $(document)
      .ready(function () {

        // fix menu when passed
        $('.masthead')
          .visibility({
            once: false,
            onBottomPassed: function () {
              $('.fixed.menu').transition('fade in');
            },
            onBottomPassedReverse: function () {
              $('.fixed.menu').transition('fade out');
            }
          })
          ;

        // create sidebar and attach to menu open
        $('.ui.sidebar')
          .sidebar('attach events', '.toc.item')
          ;

      })
      ;
  </script>
</head>

<body>

  <!-- Menu -->
  <div class="ui large top fixed hidden menu" style="background-color:rgb(224, 224, 229);">
    <div class="ui container">
      <a href="index.html" class="active item"><i class="home icon"></i>Home</a>
      <a href="https://arxiv.org/abs/2301.05880" class="item"><i class="book icon"></i>arXiv</a>
      <a href="#bib" data-target="#bib" class="item"><i class="quote right icon"></i>BibTeX</a>
      <a href="https://github.com/RUC-AIMind/TikTalk" class="item"><i class="align database icon"></i>Dataset</a>
      <a href="https://github.com/RUC-AIMind/TikTalk" class="item"><i class="github icon"></i>GitHub</a>
    </div>
  </div>

  <!-- Sidebar Menu -->
  <div class="ui vertical inverted sidebar menu">
    <a href="index.html" class="active item">
      <i class="home icon"></i>Home
    </a>


  </div>


  <!-- Page Contents -->
  <div class="pusher" >
    <div class="ui inverted vertical masthead center aligned segment">
      <div class="ui large secondary inverted pointing menu" style="background-color:#373435ff;">
        <div class="ui container">
          <a class="toc item">
            <i class="sidebar icon"></i>
          </a>
          <a href="index.html" class="active item">
            <i class="home icon"></i>Home
          </a>
        </div>
      </div>

      <div class="ui  text container" style="background-color: rgba(255,255,255,0.3); ">
        <h1 class="ui inverted header" style="color: #000000;">
          TikTalk
        </h1>
        <h2 style="color: #000000;">
          A Video-Based Dialogue Dataset for Multi-Modal Chitchat in Real World
        </h2>
        <h4 style="color: #000000;">
          <a href="https://hopelin99.github.io/" style="color: #000000;">Hongpeng Lin</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Ludan Ruan</a>&nbsp;<sup>2</sup>,
          <a href="" style="color: #000000;">Wenke Xia</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Peiyu Liu</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Jingyuan Wen</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Yixin Xu</a>&nbsp;<sup>1</sup>,
          <br />
          <a href="" style="color: #000000;">Di Hu</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Ruihua Song</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Wayne Xin Zhao</a>&nbsp;<sup>1</sup>,
          <a href="" style="color: #000000;">Qin Jin</a>&nbsp;<sup>2</sup>,
          <a href="" style="color: #000000;">Zhiwu Lu</a>&nbsp;<sup>1</sup></a>
        </h4>
        <h4 style="color: #000000;">
          <sup>1</sup> <a href="http://ai.ruc.edu.cn/english/" style="color: #000000;"> Gaoling School of Artificial Intelligence, Renmin University of China</a>
          <br />
          <sup>2</sup> <a href="http://info.ruc.edu.cn/Home/index.htm" style="color: #000000;"> School of Information, Renmin University of China</a>
        </h4>

        <div class="ui center aligned container segment" style="background-color:transparent;">
          <a href="https://arxiv.org/abs/2301.05880" class="ui primary small labeled icon button" ><i class="align book icon"></i>arXiv</a>
          <a href="#bib" class="ui primary small labeled icon button"><i class="align quote right icon"></i>BibTeX</a>
          <a href="https://github.com/RUC-AIMind/TikTalk" class="ui primary small labeled icon button"><i class="align database icon"></i>Dataset</a>
          <a href="https://github.com/RUC-AIMind/TikTalk" class="ui primary small labeled icon button"><i class="align github icon"></i>GitHub</a>
        </div>
        
      </div>
    </div>
    
    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Motivation</h1>
        <p>So, why do we want to build a new multi-modal dialogue dataset? We know that the ability to perceive and interact with multi-modal information is an important aspect of dialogue agents moving towards AGI. However, we find that previous dialogue datasets still have some gaps compared to real-world multi-modal chitchat scenarios. In real-world scenarios, participants engage in spontaneous conversations based on shared experiences of multi-modal information. Moreover, these scenarios involve diverse context that guides personalized and varied conversation content. To address these gaps, we present TikTalk.
        </p>

      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Data Construction</h1>
        <p>We collect data from the Chinese version of TikTok, which offers a wide range of video content across more than 25 categories. After watching the videos, users can leave comments and replies. As shown in the figure on the right, apart from the science category, videos depicting real-life situations constitute the majority. After acquiring the raw videos and comments, we perform various cleaning steps, such as applying a threshold for the number of likes to filter the videos and comments based on user engagement. Finally, we obtain the statistics shown in the lower-right table: Our dataset comprises a total of thirty eight thousand videos and three hundred and sixty seven dialogues. The statistics also include information on dialogue turns, video duration, and sentence length.
        </p>
        <img class="ui fluid image" src="assets/img/dataset.JPG" width="80%" />
      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Data Analysis</h1>
        <p>First, in the upper-right table, we compared the proportion of responses originating from different context across four dialogue datasets. Annotators judge whether they cannot understand a response if a specific type of context is missing. The types of context considered are vision context, audio context, text context, and external knowledge. In the example on the left side, we highlight the parts of the responses that originated from a specific context using corresponding colors. For instance, in Session 2, the level 2 response is based on vision context, while the level 3 response relies on external knowledge. It is evident that in Openvidial, the proportion of vision context is very low, which indicates that it may not be suitable for evaluating multi-modal dialogue capabilities. The vision context proportions in IGC are similar to TikTalk, while our proportion of external knowledge-based responses is higher.
          <br />Next, we explore how these proportions change as the level of utterance goese deeper. The results, depicted in the lower-right figure, demonstrate that as the number of dialogue turns increases, the proportion of information from multi-modal context in TikTalk tends to stabilize. In contrast, the proportion in IGC gradually decreases, approaching that of text-only dialogues.
        </p>
        <img class="ui fluid image" src="assets/img/teaser.JPG" width="80%" />
        <p>
          Next, we propose a multi-modal chitchat task based on TikTalk, which involves generating an ideal reply considering textual, visual, and audio context. In this task, we identify three challenges. The first challenge is the integration of multiple modalities. Since our dialogues are based on videos, the task requires perceiving and understanding various contextual cues and their interactions
          The second challenge is human interests. In a rich information environment, different individuals focus on different points of interest. Models need to capture the multi-modal information relevant to the textual context. For example, some users are interested in the reason for cutting chestnuts, while others focus on the explosion scene of popcorn. The third challenge is external knowledge. We categorize external knowledge into two types: general knowledge, such as "cutting a chestnut before roasting it prevents it from bursting during frying," and personalized knowledge related to users or bloggers, such as "she learns many skills before shooting videos."

          </p>
      </div>
    </div>



    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Baselines</h1>
        <p>
          The image shows two main multi-modal models we employed.
          The top one is the BLIP-2 framework, which utilizes adapters to connect visual features with a large language model. The bottom one is based on Maria, where we extract features from different types of context and input them into a transformer for fusion.
          For external knowledge, we introduce a conversational commonsense knowledge graph called C3KG, which matches relevant knowledge based on the textual context. For example, from the context "I don't know how someone so young can know these things," we obtain knowledge related to diligence, talent, and hard work to assist in response generation. 
        </p>
        <img class="ui fluid image" src="assets/img/model.JPG" width="80%" />
      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Experimental Results</h1>
        <p>
          The automatic metrics include measures of similarity and diversity, while the human evaluation encompass metrics for assessing rationality, specificity, multi-modality relevance, and overall quality.
        </p>
        <img class="ui fluid image" src="assets/img/results.JPG" width="80%" />
      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Case Study</h1>
        <p>
          Examples of generated response from models. (a) Two singers are singing on the stage. (b) Some people are making zongzi. (c) A video blogger is recording a new song. (d) An actress is learning how to use a prop gun from the crew.
        </p>
        <img class="ui fluid image" src="assets/img/case.JPG" width="80%" />
      </div>
    </div>


    <div class="ui vertical stripe segment" id="bib">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header ">BibTeX</h1>

        <div class="ui center aligned container segment">
          <p>  
            @article{lin2023tiktalk,<br />
              &emsp;title={TikTalk: A Multi-Modal Dialogue Dataset for Real-World Chitchat},<br />
              &emsp;author={Lin, Hongpeng and Ruan, Ludan and Xia, Wenke and Liu, Peiyu and Wen, Jingyuan and Xu, Yixin and Hu, Di and Song, Ruihua and Zhao, Wayne Xin and Jin, Qin and others},<br />
              &emsp;journal={arXiv preprint arXiv:2301.05880},<br />
              &emsp;year={2023}<br />
            }
          </p>
        </div>
      </div>
    </div>

  </div>
      
  <!-- Footer -->
  <footer style="background-color:#373435ff;">
    <div class="container">
      <div style=" text-align:center;">
          <span class="copyright" style="color:#eee;" >Copyright &copy; 2023 AIMind All rights Reserved.</span>
      </div>
    </div>
  </footer>

    
</body>

</html>
