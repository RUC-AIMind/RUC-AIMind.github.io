<!DOCTYPE html>
<html>

<head>
  <!-- Standard Meta -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">

  <!-- Site Properties -->
  <title>TeViS</title>

  <!-- You MUST include jQuery before Fomantic -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.8/dist/semantic.min.css">
  <script src="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.8/dist/semantic.min.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style type="text/css">
    .hidden.menu {
      display: none;
    }

    .masthead.segment {
      min-height: 700px;
      padding: 0em 0em;  /* munu padding */
    }

    .masthead .logo.item img {
      margin-right: 1em;
    }

    .masthead .ui.menu .ui.button {
      margin-left: 0.5em;
    }

    .masthead h1.ui.header {
      margin-top: 3em;
      margin-bottom: 0em;
      font-size: 4em;
      font-weight: normal;
    }

    .masthead h2 {
      font-size: 1.7em;
      font-weight: normal;
    }

     .ui.vertical.stripe {
      padding: 8em 0em;
    }

    .ui.vertical.stripe h3 {
      font-size: 2em;
    }

    .ui.vertical.stripe .button+h3,
    .ui.vertical.stripe p+h3 {
      margin-top: 3em;
    }

    .ui.vertical.stripe .floated.image {
      clear: both;
    }

    .ui.vertical.stripe p {
      font-size: 1.33em;
    }

    .ui.vertical.stripe .horizontal.divider {
      margin: 3em 0em;
    }

    .quote.stripe.segment {
      padding: 0em;
    }

    .quote.stripe.segment .grid .column {
      padding-top: 5em;
      padding-bottom: 5em;
    }

    .footer.segment {
      padding: 5em 0em;
    }

    .secondary.pointing.menu .toc.item {
      display: none;
    }

    @media only screen and (max-width: 700px) {
      .ui.fixed.menu {
        display: none !important;
      }

      .secondary.pointing.menu .item,
      .secondary.pointing.menu .menu {
        display: none;
      }

      .secondary.pointing.menu .toc.item {
        display: block;
      }

      .masthead.segment {
        min-height: 350px;
      }

      .masthead h1.ui.header {
        font-size: 2em;
        margin-top: 1.5em;
      }

      .masthead h2 {
        margin-top: 0.5em;
        font-size: 1.5em;
      }
    }

    p {
      text-align: justify;
      font-size: 12pt;
    }

    .masthead {
      background-image: url('assets/img/bg.png') !important;
      background-size: cover !important;
    }

    .masthead.segment {
      min-height: 300px;
    }

    .masthead h1.ui.header {
      margin-top: 0em;
    }

    .masthead .ui.tex a {
      margin-bottom: 40px;
    }

    .masthead a {
      color: #EEE;
    }

    .ui.vertical.stripe.segment {
      padding: 5em 0em;
    }
  </style>

  <script>
    $(document)
      .ready(function () {

        // fix menu when passed
        $('.masthead')
          .visibility({
            once: false,
            onBottomPassed: function () {
              $('.fixed.menu').transition('fade in');
            },
            onBottomPassedReverse: function () {
              $('.fixed.menu').transition('fade out');
            }
          })
          ;

        // create sidebar and attach to menu open
        $('.ui.sidebar')
          .sidebar('attach events', '.toc.item')
          ;

      })
      ;
  </script>
</head>

<body>

  <!-- Menu -->
  <div class="ui large top fixed hidden menu" style="background-color:rgb(224, 224, 229);">
    <div class="ui container">
      <a href="tevis.html" class="active item"><i class="home icon"></i>Home</a>
      <a href="https://arxiv.org/abs/2301.00135" class="item"><i class="book icon"></i>arXiv</a>
      <a href="#bib" data-target="#bib" class="item"><i class="quote right icon"></i>BibTeX</a>
      <a href="https://github.com/guxu313/TeViS/tree/main/metadata/MovieNet_TeViS" class="item"><i class="align database icon"></i>Dataset</a>
      <a href="https://github.com/guxu313/TeViS" class="item"><i class="github icon"></i>GitHub</a>
    </div>
  </div>

  <!-- Sidebar Menu -->
  <div class="ui vertical inverted sidebar menu">
    <a href="tevis.html" class="active item">
      <i class="home icon"></i>Home
    </a>


  </div>


  <!-- Page Contents -->
  <div class="pusher" >
    <div class="ui inverted vertical masthead center aligned segment">
      <div class="ui large secondary inverted pointing menu" style="background-color:#373435ff;">
        <div class="ui container">
          <a class="toc item">
            <i class="sidebar icon"></i>
          </a>
          <a href="tevis.html" class="active item">
            <i class="home icon"></i>Home
          </a>
        </div>
      </div>

      <div class="ui  text container" style="background-color: rgba(255, 255, 255, 0.3); ">
        <!-- <h1 class="ui inverted header" style="font-family:'Courier New', Courier, monospace; color: #0947ba;" > -->
        <!-- <h1 class="ui inverted header" style="font-family:'Playfair Display', serif; color: #0947ba;" > -->
        <h1 class="ui inverted header" style="font-family:Comic Sans MS; color: #4366b7;" >
        <!-- <h1 class="ui inverted header" style="font-family:Papyrus; color: #0947ba;" > -->
          TeViS
        </h1>
        <h2 style="color: #000000;">
          Translating Text Synopses to Video Storyboards
        </h2>
        <h4 style="color: #000000;">
          <a href="https://guxu313.github.io/" style="color: #000000;">Xu Gu</a>&nbsp;<sup>1</sup>,
          <a href="https://ycsun1972.github.io/" style="color: #000000;"> Yuchong Sun</a>&nbsp;<sup>1</sup>,
          <a href="https://github.com/nfy-dot" style="color: #000000;"> Feiyue Ni</a>&nbsp;<sup>1</sup>,
          <a href="https://cshizhe.github.io/" style="color: #000000;"> Shizhe Chen</a>&nbsp;<sup>2</sup>,
          <a href="" style="color: #000000;"> Xihua wang</a>&nbsp;<sup>1</sup>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=v5LctN8AAAAJ" style="color: #000000;"> Ruihua Song</a>&nbsp;<sup>1</sup>,
          <a href="https://boyuaner.github.io/" style="color: #000000;"> Boyuan Li</a>&nbsp;<sup>1</sup>,
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=XprSyesAAAAJ&view_op=list_works&sortby=pubdate" style="color: #000000;"> Xiang Cao&nbsp;<sup>3</sup></a>
        </h4>
        <h4 style="color: #000000;">
          <sup>1</sup> <a href="http://ai.ruc.edu.cn/english/" style="color: #000000;"> Renmin University of China</a>,
          <sup>2</sup> <a href="https://www.inria.fr" style="color: #000000;"> Inria Paris</a>,
          <sup>3</sup> <a style="color: #000000;"> Bilibili Inc.</a>
        </h4>

        <div class="ui center aligned container segment" style="background-color:transparent;">
          <a href="https://arxiv.org/abs/2301.00135" class="ui primary small labeled icon button" ><i class="align book icon"></i>arXiv</a>
          <a href="#bib" class="ui primary small labeled icon button"><i class="align quote right icon"></i>BibTeX</a>
          <a href="https://github.com/guxu313/TeViS/tree/main/metadata/MovieNet_TeViS" class="ui primary small labeled icon button"><i class="align database icon"></i>Dataset</a>
          <a href="https://github.com/guxu313/TeViS" class="ui primary small labeled icon button"><i class="align github icon"></i>GitHub</a>
        </div>
        
      </div>
    </div>

    <div class="ui vertical stripe segment" id="motivation">
      <div class="ui middle aligned stackable grid container">       

        <div class="row">
          <div class="eighteen wide column">
            <h3 class="ui header">
              <div class="ui blue horizontal big label">
                Task Definition
              </div>
              What is TeViS Task? &ensp; &#127916;<!-- <em data-emoji=':film_frames:'></em> -->
            </h3>
            <p>
              A storyboard is a roadmap for video creation which consists of shot-by-shot images to visualize key plots in a text synopsis. 
              Creating video storyboards however remains challenging which not only requires association between high-level texts and images, 
              but also demands for long-term reasoning to make transitions smooth across shots. 
              We propose a new task called Text synopsis to Video Storyboard (TeViS), 
              which aims to retrieve an ordered sequence of images to visualize the text synopsis. <br/>

              Previous works on text-to-image retrieval can only produce static images without considering the dynamics of shots in the video. 
              Text-to-video works are able to retrieve or generate videos. 
              Yet, most of them focus on short-term video clips with only a few seconds as shown in Fig. 1 (a). 
              The images in these videos are highly redundant and cannot satisfy the requirement of a video storyboard for coherent keyframes. 
              Previous works on visual storytelling works are proposed to visualize text with a sequence of images, 
              but they care more about the text-image relevancy while omitting long-term reasoning to make transition smooth across keyframes (see Fig. 1 (b)). 
              Moreover, the query texts in existing works are visually concrete and descriptive, 
              making the models less generalizable to more abstract and high-level text synopses such as the synopsis in Fig. 1 (c).
            </p>
            <img class="ui fluid image" src="assets/img/dataset.png" width="80%" />
          </div>
        </div>

        <div class="row">
          <div class="eighteen wide column">
            <h3 class="ui header">
              <div class="ui blue horizontal big label">
                Dataset
              </div>
              What is <a href="https://ruc-aimind.github.io/projects/tevis.html">MovieNet-TeViS</a> Dataset?&ensp;
            </h3>
            <p>
              In the TeViS task, we aim to retrieve an ordered sequence of 
              images from large-scale movie database as video
              storyboard to visualize an input text synopsis. For this purpose,
              we collect the <a herf="https://ruc-aimind.github.io/projects/tevis.html">MovieNet-TeViS</a> benchmark based on
              the public <a href="https://movienet.github.io/">MovieNet</a> dataset. <br/>
              <br/>
              Our collected MovieNet-TeViS dataset consists of 10,000 pairs of a synopsis sentence in English and a video storyboard, 
              i.e., a sequence of keyframes as our final dataset.  There are 45,584 keyframes in total. 
              The number of keyframes in a storyboard ranges from 3 to 11 and about 60\% storyboards consist of 3 or 4 keyframes. 
              The average number of words in a synopsis sentence is about 24. <br/>
              <br/>
              Tab. 1 presents the comparison of our dataset and related movie datasets. 
              It shows that the duration of movie clips corresponding to a description in LSMDC and MAD is only 4 seconds 
              and the average number of words in a description is only 9-12, which is much lower than ours. 
              It is impossible to extract a meaningful storyboard from such short clips. 
              Our MovieNet-TeViS and CMD give a synopsis or summary of 64-second or 132-second video segments respectively. 
              Thus we can expect the text in CMD is higher-level than that in ours, which is proved by lower avgConcreteness 
              (Please read our paper for more details). 
              Compared to CMD, our MovieNet-Tevis uses more words to describe video segments with half the duration of CMD. 
              This indicates that our text synopses provide more details than those in CMD. 
              As a start of such a challenging new task, 
              our dataset is the most appropriate in duration of video clip and semantic level of text.
            </p>
            <img class="ui fluid image" src="assets/img/dataset_tab.png" width="80%" />
          </div>
        </div>
      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Two Evaluation Subtasks</h1>
        <p>We design two evaluation settings for the TeViS task: <br/>
          <br/>
          1.	Ordering the Shuffled Keyframes Subtask:<br/>
          For a given text synopsis and its shuffled ground-truth images, 
          how well can the models order them? To measure the long-term reasoning capability of models for ordering, 
          we let the models order the ground-truth images and apply <i>Kendall' &tau;</i> to evaluate 
          how well the ordered list matches with the ground-truth list.<br/>
          <br/>
          2.	Retrieve-and-Ordering Keyframes Subtask:<br/>
          For a given text synopsis, how well can the models select the relevant images from a large set of candidates 
          and then order them? This task is more practical in real situations. 
          For this evaluation, we are given a text synopsis and a large set of candidate images. 
          The candidate images contain ground-truth images annotated by humans, 
          and other negative images which are randomly sampled from other images in the corpus. 
          The number of candidates including ground-truth and negative samples is 500. 
          We consider both retrieval and ordering performance and apply the product of <i>Recall@K</i> 
          and <i>Kendall' &tau;</i> as the final metric of this subtask. 
          Here the <i>Kendall' &tau;</i> is calculated upon the returned ground-truth images at top K only.
        </p>
      </div>
    </div>


    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Baseline Models</h1>
        <p>To provide a start point for tackling the task, 
          we propose a text-to-image retrieval module based on a pre-trained image-text model, 
          and an encoder-decoder module for ordering images. 
          A coherence-aware pre-training method is further proposed to leverage large-scale movies to improve coherence across frames for the ordering module. 
          The framework of our VQ-Trans model for the TeViS task is shown in the flow Figure.
        </p>
        <img class="ui fluid image" src="assets/img/model.png" width="80%" />
        <p> </p>
        <p>In addition to the proposed VQ-Trans model, we design three strong baselines based on CLIP for ordering as shown in Fig. C.2:
        </p>
        <img class="ui fluid image" src="assets/img/clip_baseline.png"  width="80%" />
      </div>
    </div>

    <div class="ui vertical stripe segment">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header">Experimental Results</h1>
        <p>
          Qualitative examples of different models for the ordering task on our Movie-TeViS dataset.
          The <i>kendall' &tau;</i> metrics of our approach for Ordering Task and Retrieve-and-Order Task are shown in Tab. 2 and Tab. 5. 
          To learn more details, please refer to our <a href="https://arxiv.org/abs/2301.00135">paper</a>..
        </p>

        <img class="ui fluid image" src="assets/img/res.png" width="80%"/><br/>

        <br/>
        <br/>

        <img class="ui fluid image" src="assets/img/task_all.png" width="80%"/><br/>
        <br/>
        <p>
          Nevertheless, there is still a large gap compared to human performance suggesting room for promising future work.
        </p>
      </div>
    </div>


    <div class="ui vertical stripe segment" id="bib">
      <div class="ui middle aligned stackable grid container">
        <h1 class="ui header ">BibTeX</h1>

        <div class="ui center aligned container segment">
          <p>  
            @InProceedings{Gu_2023_TeViS,<br />
            &emsp;author = {Xu Gu, Yuchong Sun, Feiyue Ni, Shizhe Chen, Xihua Wang, Ruihua Song, Boyuan Li, Xiang Cao},<br />
            &emsp;title = {TeViS: Translating Text Synopses to Video Storyboards},<br />
            &emsp;year = {2023},<br />
            &emsp;isbn = {9798400701085},<br />
            &emsp;publisher = {Association for Computing Machinery},<br />
            &emsp;address = {New York, NY, USA},<br />
            &emsp;url = {https://doi.org/10.1145/3581783.3612417},<br />
            &emsp;doi = {10.1145/3581783.3612417},<br />
            &emsp;booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},<br />
            &emsp;pages = {4968–4979},<br />
            &emsp;numpages = {12},<br />
            &emsp;keywords = {movie, datasets, storyboard, synopsis},<br />
            &emsp;location = {Ottawa ON, Canada},<br />
            &emsp;series = {MM '23}<br />
            }
          </p>
        </div>
      </div>
    </div>

  </div>
      
  <!-- Footer -->
  <footer style="background-color:#373435ff;">
    <div class="container">
      <div style=" text-align:center;">
          <span class="copyright" style="color:#eee;" >Copyright &copy; 2023 AIMind All rights Reserved.</span>
      </div>
    </div>
  </footer>

    
</body>

</html>
