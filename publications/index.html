<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="PublicationsVideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing AAAI 2023Yihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Men">
<meta property="og:type" content="website">
<meta property="og:title" content="publications">
<meta property="og:url" content="http://example.com/publications/index.html">
<meta property="og:site_name" content="AIMind">
<meta property="og:description" content="PublicationsVideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing AAAI 2023Yihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Men">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-12-08T13:39:29.000Z">
<meta property="article:modified_time" content="2023-08-03T14:01:06.278Z">
<meta property="article:author" content="AIMind All rights Reserved.">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>publications</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 5.4.2"></head>

<body class="max-width mx-auto px3 ltr">
    
    <div class="content index py4">
        
          <header id="header">
  <a href="/">
  
    
      <div id="logo" style="background-image: url(/images/logo.png);"></div>
    
  
    <div id="title">
      <h1>AIMind <div id="at">@RUC</div></h1>
    </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#" aria-label="Menu"><i class="fas fa-bars fa-2x"></i></a>
      </li>
      
        
        <li>
          <a href="/">
            Home
            
          </a>
        </li>
      
   
        
        <li>
          <a href="/team/">
            Team
            
          </a>
        </li>
      
   
        
        <li>
          <a href="/publications">
            Publications
            
          </a>
        </li>
      
   
        
        <li>
          <a href="/projects/">
            Projects
            
          </a>
        </li>
      
   
        
   
        
        <li>
          <a target="_blank" rel="noopener" href="https://gpt.by-pro.cn/">
            chatbot
            
              <i class="fas fa-robot"></i>
              
          </a>
        </li>
      
   
    </ul>
  </div>
</header>

        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  

  <div class="content" itemprop="articleBody">
      
          <h1 id="Publications"><a href="#Publications" class="headerlink" title="Publications"></a>Publications</h1><h2 id="VideoDubber-Machine-Translation-with-Speech-Aware-Length-Control-for-Video-Dubbing-AAAI-2023"><a href="#VideoDubber-Machine-Translation-with-Speech-Aware-Length-Control-for-Video-Dubbing-AAAI-2023" class="headerlink" title="VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing AAAI 2023"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2211.16934">VideoDubber: Machine Translation with Speech-Aware Length Control for Video Dubbing</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">AAAI 2023</span></h2><h3 id="Yihan-Wu-Junliang-Guo-Xu-Tan-Chen-Zhang-Bohan-Li-Ruihua-Song-Lei-He-Sheng-Zhao-Arul-Menezes-Jiang-Bian"><a href="#Yihan-Wu-Junliang-Guo-Xu-Tan-Chen-Zhang-Bohan-Li-Ruihua-Song-Lei-He-Sheng-Zhao-Arul-Menezes-Jiang-Bian" class="headerlink" title="Yihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Menezes, Jiang Bian"></a>Yihan Wu, Junliang Guo, Xu Tan, Chen Zhang, Bohan Li, Ruihua Song, Lei He, Sheng Zhao, Arul Menezes, Jiang Bian</h3><p>Video dubbing aims to translate the original speech in a film or television program into the speech in a target language, which can be achieved with a cascaded system consisting of speech recognition, machine translation and speech synthesis. To ensure the translated speech to be well aligned with the corresponding video, the length&#x2F;duration of the translated speech should be as close as possible to that of the original speech, which requires strict length control…</p>
<h2 id="Long-Form-Video-Language-Pre-Training-with-Multimodal-Temporal-Contrastive-Learning-NeurIPS-2022"><a href="#Long-Form-Video-Language-Pre-Training-with-Multimodal-Temporal-Contrastive-Learning-NeurIPS-2022" class="headerlink" title="Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning NeurIPS 2022"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.06031">Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">NeurIPS 2022</span></h2><h3 id="Yuchong-Sun-Hongwei-Xue-Ruihua-Song-Bei-Liu-Huan-Yang-Jianlong-Fu"><a href="#Yuchong-Sun-Hongwei-Xue-Ruihua-Song-Bei-Liu-Huan-Yang-Jianlong-Fu" class="headerlink" title="Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, Jianlong Fu"></a>Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, Jianlong Fu</h3><p>Large-scale video-language pre-training has shown significant improvement in video-language understanding tasks. Previous studies of video-language pretraining mainly focus on short-form videos (i.e., within 30 seconds) and sentences, leaving long-form video-language pre-training rarely explored. Directly learning representation from long-form videos and language may benefit many long-form video-language understanding tasks…</p>
<h2 id="Multi-Modal-Experience-Inspired-AI-Creation-ACM-Multimedia-2022"><a href="#Multi-Modal-Experience-Inspired-AI-Creation-ACM-Multimedia-2022" class="headerlink" title="Multi-Modal Experience Inspired AI Creation ACM Multimedia 2022"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2209.02427">Multi-Modal Experience Inspired AI Creation</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">ACM Multimedia 2022</span></h2><h3 id="Qian-Cao-Xu-Chen-Ruihua-Song-Hao-Jiang-Guang-Yang-Zhao-Cao"><a href="#Qian-Cao-Xu-Chen-Ruihua-Song-Hao-Jiang-Guang-Yang-Zhao-Cao" class="headerlink" title="Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao"></a>Qian Cao, Xu Chen, Ruihua Song, Hao Jiang, Guang Yang, Zhao Cao</h3><p>AI creation, such as poem or lyrics generation, has attracted increasing attention from both industry and academic communities, with many promising models proposed in the past few years. Existing methods usually estimate the outputs based on single and independent visual or textual information. However, in reality, humans usually make creations according to their experiences, which may involve different modalities and be sequentially correlated. To model such human capabilities, in this paper, we define and solve a novel AI creation problem based on human experiences…</p>
<h2 id="Self-supervised-Context-aware-Style-Representation-for-Expressive-Speech-Synthesis-Interspeech-2022"><a href="#Self-supervised-Context-aware-Style-Representation-for-Expressive-Speech-Synthesis-Interspeech-2022" class="headerlink" title="Self-supervised Context-aware Style Representation for Expressive Speech Synthesis Interspeech 2022"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.12559">Self-supervised Context-aware Style Representation for Expressive Speech Synthesis</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">Interspeech 2022</span></h2><h3 id="Yihan-Wu-Xi-Wang-Shaofei-Zhang-Lei-He-Ruihua-Song-Jian-Yun-Nie"><a href="#Yihan-Wu-Xi-Wang-Shaofei-Zhang-Lei-He-Ruihua-Song-Jian-Yun-Nie" class="headerlink" title="Yihan Wu, Xi Wang, Shaofei Zhang, Lei He, Ruihua Song, Jian-Yun Nie"></a>Yihan Wu, Xi Wang, Shaofei Zhang, Lei He, Ruihua Song, Jian-Yun Nie</h3><p>Expressive speech synthesis, like audiobook synthesis, is still challenging for style representation learning and prediction. Deriving from reference audio or predicting style tags from text requires a huge amount of labeled data, which is costly to acquire and difficult to define and annotate accurately. In this paper, we propose a novel framework for learning style representation from abundant plain text in a self-supervised manner. It leverages an emotion lexicon and uses contrastive learning and deep clustering…</p>
<h2 id="AdaSpeech-4-Adaptive-Text-to-Speech-in-Zero-Shot-Scenarios-Interspeech-2022"><a href="#AdaSpeech-4-Adaptive-Text-to-Speech-in-Zero-Shot-Scenarios-Interspeech-2022" class="headerlink" title="AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios Interspeech 2022"></a><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.00436">AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">Interspeech 2022</span></h2><h3 id="Yihan-Wu-Xu-Tan-Bohan-Li-Lei-He-Sheng-Zhao-Ruihua-Song-Tao-Qin-Tie-Yan-Liu"><a href="#Yihan-Wu-Xu-Tan-Bohan-Li-Lei-He-Sheng-Zhao-Ruihua-Song-Tao-Qin-Tie-Yan-Liu" class="headerlink" title="Yihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, Tie-Yan Liu"></a>Yihan Wu, Xu Tan, Bohan Li, Lei He, Sheng Zhao, Ruihua Song, Tao Qin, Tie-Yan Liu</h3><p>Adaptive text to speech (TTS) can synthesize new voices in zero-shot scenarios efficiently, by using a well-trained source TTS model without adapting it on the speech data of new speakers. Considering seen and unseen speakers have diverse characteristics, zero-shot adaptive TTS requires strong generalization ability on speaker characteristics, which brings modeling challenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS system for high-quality speech synthesis. We model the speaker characteristics systematically to improve the generalization on new speakers…</p>
<h2 id="Text2Poster-Laying-Out-Stylized-Texts-on-Retrieved-Images-ICASSP-2022"><a href="#Text2Poster-Laying-Out-Stylized-Texts-on-Retrieved-Images-ICASSP-2022" class="headerlink" title="Text2Poster: Laying Out Stylized Texts on Retrieved Images ICASSP 2022"></a><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9747465">Text2Poster: Laying Out Stylized Texts on Retrieved Images</a> <span style="font-size:0.4em;color:white;background:rgba(212,70,74,1);padding:0.5em">ICASSP 2022</span></h2><h3 id="Chuhao-Jin-Hongteng-Xu-Ruihua-Song-Zhiwu-Lu"><a href="#Chuhao-Jin-Hongteng-Xu-Ruihua-Song-Zhiwu-Lu" class="headerlink" title="Chuhao Jin, Hongteng Xu, Ruihua Song, Zhiwu Lu"></a>Chuhao Jin, Hongteng Xu, Ruihua Song, Zhiwu Lu</h3><p>Poster generation is a significant task for a wide range of applications, which is often time-consuming and requires lots of manual editing and artistic experience. In this paper, we propose a novel data-driven framework, called Text2Poster, to automatically generate visually-effective posters from textual information…</p>

        
  </div>
</article>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2022-2023
    AIMind All rights Reserved.
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
